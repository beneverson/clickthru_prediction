{
 "metadata": {
  "name": "",
  "signature": "sha256:3dda7c3d571da593eda74107cbd25ce0c3e0c5beaef3eb675bab3dab5cb6e8b1"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "from sklearn.feature_extraction import FeatureHasher\n",
      "from scipy.sparse import hstack, coo_matrix\n",
      "from sklearn import metrics\n",
      "from sklearn.cross_validation import train_test_split\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "import datetime\n",
      "from sklearn.preprocessing import StandardScaler"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Read a sample of the data:\n",
      "# This sample consists of 50k records randomly sampled from the data and\n",
      "# was produced with the subsample package. \n",
      "# \n",
      "parse = lambda x: datetime.datetime.strptime(x, '%y%m%d%H')\n",
      "df = pd.read_csv('data/train_sample.csv', parse_dates=[2], date_parser=parse, dtype=str, nrows=50000) \n",
      "# only read in a small subset"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# generate a sparse matrix from the dataframe 'dataframe'\n",
      "def generate_feature_matrix_for_categoricals(feaures_to_use, dataframe):\n",
      "    fh = FeatureHasher(n_features=2**18, input_type=\"string\")\n",
      "    feature_matrices = []\n",
      "    for feature_name in feaures_to_use:\n",
      "        feature_matrices.append(fh.transform(k for k in dataframe[feature_name].values))\n",
      "    return hstack(feature_matrices)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def make_submission_file(prediction_model, output_file_name, chunksize=10000, test_file_name='data/test_rev2'):\n",
      "    # read the test data in chunks\n",
      "    test_data = pd.read_csv(test_file_name, parse_dates=[1], date_parser=parse,\n",
      "                            dtype=str, iterator=True, chunksize=chunksize)\n",
      "    # iterate through chunks, predict on each\n",
      "    submission_df = pd.DataFrame(columns=['id','click'])\n",
      "    for chunk in test_data:\n",
      "        # determine and join the feature matrices\n",
      "        X_chunk_categorical = generate_feature_matrix_for_categoricals(categorical_features, chunk)\n",
      "        X_chunk_hour_dirty = [[float(d.month), float(d.day), \n",
      "                         float(d.hour), float(d.hour ** 2), \n",
      "                         float(d.weekday()),  float(d.weekday() ** 2)] for d in chunk['hour']]\n",
      "        scaler = StandardScaler()\n",
      "        X_hours_scaled = coo_matrix(scaler.fit_transform(X_chunk_hour_dirty))\n",
      "        X_chunk = hstack([X_chunk_categorical,X_hours_scaled])\n",
      "        # determine predictions\n",
      "        y_predicted = prediction_model.predict_proba(X_chunk)\n",
      "        # create a new df from this data\n",
      "        predicted_chunk = pd.DataFrame()\n",
      "        predicted_chunk['id'] = chunk['id']\n",
      "        predicted_chunk['click'] = [pred[1] for pred in y_predicted]\n",
      "        # append this predicted chunk to the output df\n",
      "        submission_df = submission_df.append(predicted_chunk, ignore_index=True)\n",
      "    submission_df.to_csv(output_file_name, index=False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Feature selection: \n",
      "# try site_category, site_domain, app_category, app_domain, \n",
      "# device_type, device_geo_country, hour, weekday, month\n",
      "# use the feature hash trick to get a proper feature matrix\n",
      "categorical_features = [u'C1', u'banner_pos', u'site_id', u'site_domain', u'site_category', u'app_id', \n",
      "                        u'app_domain', u'app_category', u'device_id', u'device_ip', \n",
      "                        u'device_os', u'device_make', u'device_model', u'device_type', \n",
      "                        u'device_conn_type', u'device_geo_country', u'C17', u'C18', u'C19', \n",
      "                        u'C20', u'C21', u'C22', u'C23', u'C24']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# determine the datetime features\n",
      "X_hours = [[float(d.month), float(d.day), float(d.hour), \n",
      "            float(d.hour ** 2), float(d.weekday()),  float(d.weekday() ** 2)] for d in df['hour']]\n",
      "# scale and convert to sparse matrix\n",
      "scaler = StandardScaler()\n",
      "X_hours_scaled = coo_matrix(scaler.fit_transform(X_hours))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# determine the categorical feature matrix\n",
      "X_categorical = generate_feature_matrix_for_categoricals(categorical_features, df)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# our final X and y\n",
      "X = hstack([X_categorical,X_hours_scaled])\n",
      "y = df['click'].apply(lambda x: int(x)).values"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# split into training / test sets\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# using logistic regression with l2 penalty, fit the data\n",
      "clf = LogisticRegression(penalty=\"l2\", C=100)\n",
      "clf.fit(X_train,y_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 10,
       "text": [
        "LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
        "          intercept_scaling=1, penalty='l2', random_state=None, tol=0.0001)"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# predict on the test set\n",
      "y_pred = clf.predict(X_test)\n",
      "print metrics.classification_report(y_test, y_pred)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "             precision    recall  f1-score   support\n",
        "\n",
        "          0       0.85      0.99      0.91     12594\n",
        "          1       0.56      0.05      0.10      2406\n",
        "\n",
        "avg / total       0.80      0.84      0.78     15000\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "make_submission_file(clf, 'submission2_110614.csv')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}